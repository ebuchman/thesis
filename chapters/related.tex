\chapter{Related Work}
\label{ch:related}

Byzantine consensus has a rich history that spans cryptography, distributed computing, and economics,
but the socio-economic context for its products to be deployed in industry has not existed until recently,
at least not outside of traditionally critical real-time systems like aircraft control \cite{draper_lab}.
On the one hand, the invention of Bitcoin and the coining of the term ``blockchain'' popularized the notion
of a distributed ledger not controlled by a single entity, using cryptography and aligned economic incentives to 
preserve safety in the face of Byzantine faults.
On the other, the continued commoditization of servers, in the form of ``The Cloud'', and the invention of Raft, 
have popularized distributed computing in mainstream developer culture, 
and brought renewed attention to distributed consensus algorithms as co-ordination hubs in large-scale deployments. 

At the intersection are a collection of solutions, typically geared for banking and financial applications,
but also for governance, logistics, and other general forms of co-ordination, 
that draw on classic academic BFT modified and modernized in various ways.
This chapter reviews the history and diversity of these ideas, with the goal of providing a rich context within which to 
understand the blockchain phenomenon.

\section{Beginnings}

Distributed algorithms first emerged in the late 19th century in the telecommunications and railroad industries,
in attempts to effectively handle multiple concurrent message streams in a transmission, 
or multiple trains on the same set of tracks.

Academic work on the subject appears to have been launched officially by the seminal work
of Edsger Dijkstra on the mutual exclusion problem \cite{mutex}, and
of Tony Hoare on models for describing communicating processes \cite{csp}.

A host of concurrency problems with catchy names were popularized around this time,
including the cigarette smokers problem \cite{cigarette_smokers}, where smokers sit around a table, 
each with a different ingredient, and must successfully roll a full cigarette,
the dinning philosophers problem \cite{dining_philosophers},
where philosophers sitting around a table must take turns eating and thinking,
but each can only eat while its neighbours are thinking,
and the two-generals or co-ordinated attack problem \cite{gettier},
where two generals must co-ordinate from afar to attack an enemy city at the same time.

These problems served to put the focus on synchronization primitives
such as semaphores, mutexes, and communication channels,
and would lay the groundwork for a number of advancements over the coming decades.

\subsection{Faulty Things}

Fault tolerant distributed computing effectively emerged in the late seventies 
out of the effort to utilize microprocessors for aircraft control, resulting in a number of early systems \cite{sift,ftmp}.
Today, it is standard for NASA to conduct BFT research \cite{miner2004unified}, 
and for commercial aircraft to use BFT systems, such as the SAFEbus \cite{hoyme1993safebus}.

Many systems, however, do not require tolerance to Byzantine faults as they are run in controlled environments,
where there is presumably no malicious behaviour and the code is written correctly.
In these circumstances, which are common in data-centers managed by large companies like Google or Amazon,
fault tolerant computing is used to defend against various faults,
whether it be a break in a network link, power failure in a server rack, or a dead harddrive.

\subsection{Clocks}

The problem of distributed consensus, however, did not formally emerge until Leslie Lamport introduced it in his 
``Time, clocks, and the ordering of events in a distributed system'' \cite{clocks}.
In that work, Lamport demonstrated how a partial ordering of events emerges from a definition of causality based on communication \cite{clocks}.
That is, events occurring in concurrent processes, between communication events, 
effectively happen at the same time, as they cannot influence one another.
Thus, a system of logical clocks can be defined based on the individual sequential processes 
and the fact that messages are sent before they are received.
Events can then be totally ordered by assigning any arbitrary but consistent total ordering above the partial ordering,
for instance by assigning each process in the system an index and ordering events which happen at the same logical time by the
index of the process in which they happen.
The algorithm is quite simple, requiring each process to hear from each other process in order to determine the order of events.

Lamport's work established time as a principle obstacle to designing fault tolerant distributed systems,
as synchronizing clocks across geographical locations requires the communication of messages 
which is ultimately limited by the speed of light.
This formulation of the problem has close ties to the relativism of modern physics,
wherein frames of reference are relative to an observer and the speed of light imposes a constraint on information propagation.

\subsection{FLP}
As discussed in Chapter \ref{ch:motivation}, 
one of the primary factors in designing consensus algorithms are assumptions made about 
network and/or processor synchrony. 
A synchronous network is one in which messages are delivered within some fixed, 
known amount of time. 
Similarly, synchronous processors are one whose clocks stay within some fixed, 
known number of ticks of each other.
In the early days of consensus research, the distinction was not well characterized, 
though the close relationship between asynchrony and crash failures is apparent even in \cite{clocks}.
Lamport's original consensus algorithm is able to operate in asynchronous environments, 
so long as all messages are eventually delivered from each process.
However, the algorithm is obviously not fault tolerant as the failure of just a single process can halt the algorithm forever.

The intuition behind a single failure thwarting a consensus protocol was given formal ground by Fischer, Lynch, and Patterson,
who proved the impossibility of deterministic distributed consensus in asynchronous environments even if a single process fails \cite{flp}.
The result does not apply to synchronous contexts, 
as assumptions about network synchrony allow processors to detect failures using timeouts, 
such that if a process does not respond within some given amount of time it is assumed to have crashed.
Furthermore, the result applies to deterministic consensus protocols only, 
as its proof relies on the moment when the network goes deterministically from a bivalent state, 
where not all processes hold the same value, to a univalent one, where they do.
Since the point of transition is a deterministic point in time, 
consensus fails if a single process crashes at that opportune moment.

\subsection{Common Coin}

The FLP result became something of a warning bell to distributed systems scientists, 
establishing a clear impossibility result at the heart of the emerging field.
Later, the approach would be generalized to derive many more impossibility results \cite{impossibility},
and significant academic effort would be expended on relaxing either the synchrony or determinism assumptions to derive algorithms which circumvent the result.

In particular, in a short note, 
Ben Or demonstrated how an algorithm which includes a simple amount of non-determinism can circumvent the FLP result \cite{free-choice}.
The algorithm is tolerant to faults of up to half of the processes in asynchronous environments.
Essentially, in trying to reach consensus on the value of a single bit, 
if a process does not receive votes from a majority for the same value, it randomly changes the value it votes for the next round.
With everyone changing values, eventually more than half of them will vote the same value.
This approach came to be known as a \emph{common coin},
due to the resemblance of the procedure to communally flipping a coin to obtain a shared value.

The problem with Ben Or's common coin is that, in the asynchronous case,
the algorithm requires a number of rounds exponential in the number of validators.
This was quickly rectified in a follow up by Rabin, who showed how a common coin
could be constructed using secret sharing, as pioneered by Shamir \cite{shamir1979share},
to achieve consensus in a fixed number of rounds \cite{rabin1983randomized}.
The approach is useful for BFT as well, and is discussed more fully in that context in a later section.

\subsection{Transaction Processing}

Parallel to the development of fault tolerant consensus algorithms was the emergence of the first commercial database systems.
While they did not at first use the consensus protocols being developed, 
they built atop the growing body of work in distributed computing and concurrency.
In particular is the seminal work of Jim Gray, who introduced the term \emph{transaction} 
as an atomic unit of work in a database system \cite{gray1981transaction}. 
That is, a transaction is either applied in full or not at all.

Gray also introduced other classic features of modern databases,
such as the principles of Atomicity, Consistency, Isolation, and Durability, 
which come part and parcel with the transaction concept \cite{gray1981transaction},
and the use of write-ahead-logs, for logging transactions to disk before they are executed
in order to recover from faults occurring during transaction execution \cite{gray1978notes}.

In a distributed database setting, this work on transactions, atomicity, and consistency
led to a series of approaches for database replication centered around the notion of an \emph{atomic commit},
wherein a transaction is replicated atomically across all machines.
These approaches are known as two-phase-commit \cite{gray1978notes}, and its non-blocking alternative,
three-phase-commit \cite{skeen1983formal}.

Both two-phase and three-phase commit protocols work only in a synchronous setting,
where crash failures can be detected, and utilize a co-ordinator process that serves as leader for the protocol.

\subsection{Broadcast Protocols}

The problem of ordering a set of transactions in a distributed setting subject to faults has also been known as 
the problem of \emph{total order broadcast} or \emph{atomic broadcast} \cite{defago2004total}.
In particular, an atomic broadcast protocol is expected to satisfy the following properties:

\begin{itemize}
\item{Validity}: if a correct process broadcasts a message, it is eventually received.
\item{Agreement}: if a process receives a message, then all correct processes eventually receive that message.
\item{Integrity}: a message is received at most once by a process, and only if it was sent by another process.
\item{Total Order}: if two messages are received by two processes, the messages are received in the same order.
\end{itemize}

Atomic broadcast is built above a simpler primitive, known as \emph{reliable broadcast}, 
the difference being that atomic broadcast ensures the messages are totally ordered,
while reliable broadcast ensures only that the messages are delivered.
That is, reliable broadcast satisfies all the above properties except the final one.
A taxonomy and survey of solutions to the problem is provided in \cite{defago2004total}.

\section{Byzantine}
Many fault tolerant protocols focus only on crash failures, as they are the most common,
while much less attention has been given to the problem of potentially arbitrary, including malicious,
behaviour of software. This more general problem is known as Byzantine Fault Tolerance.

\subsection{Byzantine Generals}

Lamport introduced the problem of Byzantine Fault Tolerance in \cite{pease1980reaching},
but gave the problem its name in a later paper by making an analogy with the problem faced
by the Byzantine army in co-ordinating to attack an enemy city \cite{lamport1982byzantine}.
The army is composed of multiple divisions, each of which is led by a general.
Communication between generals happens only via messenger.
How can the generals agree on a common plan of action if one or some of the generals is a traitor?

The original paper provides the first proof that to tolerate $f$ Byzantine faults,
a system must have at least $3f+1$ nodes. 
The intuition behind this result was depicted in \ref{fig:byzantine} and discussed 
throughout Chapters \ref{ch:motivation} and \ref{ch:tendermint}.
A number of algorithms are provided in both papers as the first solutions to the problem,
though they are designed to work only in the synchronous case, where the absence of a message can be detected.

\subsection{Randomized Consensus}

Asynchronous Byzantine consensus saw its first solution in the form of the common coins
introduced by Ben Or \cite{free-choice} and Rabin \cite{rabin1983randomized}.
However, neither solution achieves optimal Byzantine fault tolerance of $3f+1$ machines for $f$ faults.
Ben Or's solution requires $5f+1$ machines, while Rabin's requires $10f+1$ machines.
The solution was iteratively improved to achieve optimal Byzantine agreement with low overhead \cite{feldman1988optimal,canetti1993fast,cachin2000random}.

\subsection{Partial Synchrony}

The next major advancement in BFT came in the form of the so called \emph{DLS} consensus algorithms,
named after the authors Dwork, Lynch, and Stockmeyer \cite{dls}.
The innovation of DLS was to define a middle ground between synchrony and asynchrony called \emph{partial synchrony}.
Recall from Chapter \ref{ch:motivation} that a synchrony assumption is one which states that messages 
are received within some known, finite amount of time, 
or that processor clocks only deviate from each other by some finite number of ticks.
The secret to partial synchrony is to suppose one of the following:

\begin{itemize}
\item{Messages are guaranteed to be delivered within some fixed but unknown amount of time.}
\item{Messages are guaranteed to be delivered within some known amount of time, beginning an unknown amount of time in the future.}
\end{itemize}

The DLS algorithm proceeds via a series of rounds, each of which is divided into \emph{trying} and \emph{lock-release} phases.
Each round has a corresponding proposer, and processes can \emph{lock} on a value at a round if they think
the proposer will propose that value.
A round begins with processes gossiping the values they deem acceptable.
The proposer will propose a value if it has heard from at least $N - f$ processes that the value is acceptable.
Any process which receives the proposed value should lock on it, and send an acknowledgement message
that it has done so.
If the proposer receives acknowledgement from $f+1$ processes, it commits the value.

Variations on the basic protocol are discussed for different combinations of assumptions,
and many proofs are provided of its soundness.
Despite its success, however, DLS algorithms were never widely adopted for BFT.
Tendermint's original design was based on DLS, in particular the version which assumes
a partially synchronous network but synchronous processor clocks.
In practice, due to the use of protocols like the Network Time Protocol (NTP), 
synchronized clocks may be a fair assumption.
However, NTP is vulnerable to a number of attacks,
and protocols which assume synchronous clocks can be slow to recover from crash faults.
In the summer of 2015, the core Tendermint consensus protocol was redesigned to be more fully asynchronous,
as described in Chapter \ref{ch:tendermint},
and has thus come to more closely resemble another BFT algorithm,
known as Practical Byzantine Fault Tolerance (PBFT).

\subsection{PBFT}

PBFT was introduced in 1999 \cite{pbft}, and was widely hailed as the first practical BFT algorithm,
suitable for use in asynchronous networks,
though it does in fact make weak synchrony assumptions which can be violated by a careful adversary \cite{honeybadger}.
PBFT proceeds through a series of views, 
where each view has a proposer, known as a primary,
that is selected in round-robin order.
The primary receives requests from clients,
assigns them a sequence number, and broadcasts a signed \emph{pre-prepare}
messages to the other processes containing the view and sequence numbers.
Replicas accept the \emph{pre-prepare} message if they have not already accepted one for the same
view and sequence numbers, assuming the message is for the current view and signed by the correct primary.

Once a \emph{pre-prepare} is accepted, a replica broadcasts a signed \emph{prepare} message.
A replica is said to be \emph{prepared} for a given client request when it has received $2f$ \emph{prepare}
messages for that request, with the same view and sequence number.
The combination of \emph{pre-prepare} and \emph{prepare} ensure a total order on the requests in a single view,
according to their sequence number. 
Once a replica is prepared, it broadcasts a signed \emph{commit} message,
which is accepted so long as its properly signed and the view is correct.
When a replica accepts a \emph{commit} message, it runs the client request against the state machine and returns the result to the client.

PBFT employs an additional mechanism to facilitate view changes in the event the primary is faulty.
Replicas maintain a timeout, which restarts every time they receive a new client request, 
and terminates when a \emph{pre-prepare} is received for that request.
If no \emph{pre-prepare} is received, the replica  times out, and triggers the view change protocol.
View change is subtle and somewhat complicated as it requires consensus that the view should be changed, 
and all client requests since the last commit must be brought into the new view.

Tendermint side-steps these issues through the use of blocks and by changing proposers every block,
allowing a proposer to be skipped using the same mechanism used to commit the proposed block.
Furthermore, the use of blocks allows Tendermint to include the set of \emph{pre-commit} 
messages from one block in the next block, removing the need for an explicit \emph{commit} message.

\subsection{BFT Improvements} 

Many improvements have been proposed for PBFT since it was published.
Some of these focus on so-called \emph{optimistic execution}, 
where transactions are executed before they are committed in order to provide a low-latency,
optimistic reply to clients \cite{kotla2007zyzzyva,garcia2011efficient}.
The trouble with these approaches is that the responsibility of managing inconsistency is relegated to the client,
while presumably the reason they used a consistent consensus protocol in the first place was to avoid that responsibility.
Alternatively, this may be a useful approach in low-fault circumstance.
The phenomenon is referred to as \emph{zero-conf transactions} in Bitcoin and is widely warned against,
given Bitcoin's weak consistency guarantees.

Others have focused on the possibility of running indepedent transactions concurrently to achieve higher throughputs \cite{kotla2004high}.
This is the approach that has begun to be researched in the blockchain community, especially by Ethereum, in order to produce a scalable blockchain architecture.

\section{Non-Byzantine}

In parallel to the BFT algorithms, a number of non-BFT algorithms have emerged, and a 
number of important highly available internet services have been built on top of them.

\subsection{Paxos}

It is often said in consensus science that there is only one consensus algorithm, and it is Paxos.
This is on the one hand a statement of the significance of the Paxos algorithm to the field,
and on the other a reflection on the universal foundation of consensus protocols,
which is in every case ``Paxos-like''.

Lamport introduced Paxos in the early nineties, though the article was not accepted for publication until 
almost a decade later \cite{paxos}.
Many have pointed out that the algorithm is actually quite similar to Viewstamped Replication,
published in the late eighties \cite{oki1988viewstamped},
and that the two represent independent discovery of the same protocol.

The protocols are quite similar to PBFT, which came after them, 
but require only $2f+1$ machines to tolerate $f$ faults as they are not BFT.
Another similar protocol, the Zookeeper Atomic Broadcast protocol (ZAB) \cite{junqueira2011zab}
was developed for the Apache Zookeeper distributed key-value store.
The similarities and differences of each algorithm are illuminated in \cite{van2015vive}.

\subsection{Raft} 

Non-BFT consensus science received a major improvement with the introduction of Raft \cite{raft},
which was designed from the ground up to be \emph{understandable}, 
and which even proved itself to be more understandable than Paxos through a user survey \cite{raft_thesis}.

Raft is similar in spirit to Paxos and Viewstamped Replication, but it emphasizes replicating a transaction log, 
rather than a single bit, and introduces randomization for more efficient leader elections.
Furthermore, Raft's safety guarantees have been formally proven using the Coq proof assistant \cite{woos2016planning}
and a framework built above Coq, called Verdi, for formally verifying distributed systems \cite{wilcox2015verdi}.

\section{Blockchain}

This thesis was motivated by the introduction of blockchain technology, which emerged in the form of Bitcoin,
and has since seen many iterations.
Few have succeeded in putting the blockchain in context of classical consensus science until recently \cite{vukolic11quest,cachin2016non,honeybadger}.

\subsection{Bitcoin} 

Bitcoin was the first blockchain, introduced in \cite{bitcoin}.
It solved the atomic broadcast problem in a public, adversarial setting through a clever use of economics.
In particular, the order of transactions comes in blocks proposed by those who solve partial hash collisions,
where the data being hashed is the block of transactions.
Since computing partial hash collisions is expensive, requiring brute force search in a large space,
the effort is subsidized by the issuance of a currency, known as bitcoins, with every block.
The protocol has been wildly successful, with the currency achieving a market capitalization
of roughly five billion dollars (USD), and with many clones of the original that have market capitalizations in the millions.

However, Bitcoin is not without its issues. A number of design flaws make the protocol cumbersome and difficult 
for application developers to work with it.
Furthermore, a number of academic works have shed light on incentive incompatibilities in the protocol,
weakening widely held assumptions about the protocol's security \cite{eyal2014majority,courtois2014subversive}.

Numerous approaches have been proposed to improve Bitcoin,
including those that change the nature of the partial hash collision function \cite{miller2015nonoutsourceable},
those that change the nature of leadership election in the protocol to improve many features of the economics and underlying performance \cite{eyal2015bitcoin}
and those that aim to augment the protocol in an effort to achieve scalability \cite{back2014enabling,poon2015bitcoin}.

\subsection{Ethereum}

Ethereum was introduced by Vitalik Buterin as a solution to the proliferation of cryptocurrencies that followed Bitcoin,
with different varieties of features \cite{buterin2013ethereum}.
Ethereum sought a more pure mandate: to have no features.
Instead, Ethereum provides a Turing complete virtual machine, the Ethereum Virtual Machine (EVM), for transaction execution above the consensus,
and provides a means for users to upload code to the EVM that can execute upon the processing of future transactions.
So-called \emph{smart contracts} \cite{} offer the promise of automatically enforced execution of code in a public setting, 
using strong cryptography and BFT replication. 
The Ethereum project was successful in one of the largest crowdfunds to date, over \$18 million USD, 
and the market capitalization of its native token, ether, which is used to pay for transaction execution and code uploads,
has since reached \$1 billion USD.

Ethereum currently uses a modified form of Proof-of-Work called Greedy Heaviest Observed Sub Tree (GHOST) \cite{ghost},
but is planning to move to a more secure economic consensus algorithm modeled around Proof of Stake.

\subsection{Proof-of-Stake}
...

\subsection{HyperLedger}

The success of Bitcoin, Ethereum, and other cryptocurrencies has inspired an increasingly diverse cross section of society,
including regulators, bankers, business executives, auditors, account managers, logisticians, and more.
In particular, a recent project under the Linux Foundation, spearheaded by IBM and a new blockchain-based company called Digital Asset Holdings (DAH), 
seeks to provide a unified blockchain architecture for industrial applications. The project is called HyperLedger,
after a company with the same name, providing a rudimentary implementation of a PBFT-based blockchain, was acquired by DAH.

Two contributions to the HyperLedger initiative are particularly relevant.
The first is the combination of Juno and Hopper by the team at JP Morgan.
Juno is an implementation of Tangaroa, a BFT version of Raft \cite{tangaroa},
Hopper is a new virtual machine design,
based on linear logic \cite{girard1987linear} and dependent type systems \cite{bove2009dependent},
that aims to provide an execution environment for smart contract systems equipped with a formal logic
for making and proving statements about the state of the system, or the behaviour of a contract.
Both Juno and Hopper are written in Haskell.

The other project is the OpenBlockchain by IBM, a PBFT-based blockchain written in Go,
sporting an application state that supports the deployment of arbitrary docker containers.
Since an arbitrary docker container may contain non-determinism, their PBFT implementation
was modified with additional steps to preserve safety in the face of possibly non-deterministic execution \cite{cachin2016non}.

Another relevant contribution from IBM is a recent review paper, similar in spirit to this chapter \cite{vukolic11quest}.

\subsection{HoneyBadgerBFT}

All Paxos-like consensus protocols, including Raft, PBFT, and Tendermint, 
despite functioning well in asynchronous environments, are not strictly asynchronous.
This is because each one uses a timeout somewhere in the protocol, typically to detect faulty leaders.
On the other hand, randomized consensus protocols like the common coin offer solutions that work in 
a fully asynchronous context, with no timeouts.

All consensus protocols rely one way or another on the eventual delivery of messages.
The assumption of asynchrony simply states that there is no upper bound on when a message will be delivered.
Most of the time, networks act synchronous, in the sense that most messages are delivered within some bound.
The difference between a fully asynchronous protocol and one with timeouts is that 
an asynchronous protocol can \emph{always make progress} during times when the network is behaving synchronously.
This point is illustrated clearly in \cite{honeybadger}, which introduces HoneyBadgerBFT, 
the first fully asynchronous blockchain design, based on common coin consensus.

An adversary with arbitrary control over the network, 
and the ability to crash any one node at a time,
can cause PBFT to halt for arbitrarily long.
This can be done by crashing the current primary/proposer/leader during times when the network is synchronous,
and bringing it back for periods of asynchrony.
The network still eventually delivers messages, with some average synchrony,
but with precise timing can stop all system progress.
The experiment is carried out on PBFT directly in \cite{honeybadger}, and would work similarly against Tendermint.

The solution to this problem is HoneyBadgerBFT, a fully asynchronous atomic broadcast protocol \cite{honeybadger}.
HoneyBadgerBFT utilizes a series of cryptographic techniques, including secret sharing, erasure coding, 
and threshold signatures to design a high performance asynchronous BFT consensus protocol.
However, it requires a trusted dealer for initial setup and for validator changes, 
and it relies on relatively new cryptographic assumptions about the hardness of certain problems that have 
yet to withstand the test of time.

\section{Conclusion}

Tendermint emerges from and complements a rich history of consensus science which spans the gamut of synchrony and fault-tolerance assumptions. 
The invention of the blockchain and of Raft have rekindled the fire in consensus research and spawned a new generation 
of protocols and software for co-ordination over the internet.
